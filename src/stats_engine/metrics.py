# PROJE METRÄ°K VE PERFORMANS DEÄERLENDÄ°RME MODÃœLÃœ
#
# Bu modÃ¼l, endÃ¼striyel gÃ¼venlik odaklÄ± bir yapay zeka modelinin performansÄ±nÄ± deÄŸerlendirmek iÃ§in tasarlanmÄ±ÅŸtÄ±r.
# AmaÃ§: Modelin endÃ¼striyel gÃ¼venilirliÄŸini (Recall, ROC-AUC) kanÄ±tlamaktÄ±r.
#
# METRÄ°K STRATEJÄ°SÄ°:
# 1. Recall (DuyarlÄ±lÄ±k): Ana metrik. False Negative ayÄ±klama.
# 2. Confusion Matrix: Modelin nerede hata yaptÄ±ÄŸÄ±nÄ± gÃ¶steren ÅŸema.
# 3. Accuracy (DoÄŸruluk): Genel baÅŸarÄ± gÃ¶stergesi (Ancak dengesiz veride yanÄ±ltÄ±cÄ± olabilir).
# 4. F1 Score: Precision ve Recall arasÄ±ndaki denge.
# 5. ROC-AUC: SÄ±nÄ±flandÄ±rma kalitesi.

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    confusion_matrix, 
    recall_score, 
    accuracy_score, 
    f1_score, 
    roc_auc_score, 
    classification_report
)



class PerformanceEvaluator:
    def __init__(self):
        self.y_true = []      # Ground truth (0: Normal, 1: Failure)
        self.y_pred = []      # Model prediction
        self.y_prob = []      # Confidence score / Probability

    def add_record(self, ground_truth, prediction, probability=None):
        """
        Appends a single simulation step result to the history.
        """
        self.y_true.append(ground_truth)
        self.y_pred.append(prediction)
        
        # Use prediction as probability if no specific score is provided
        if probability is not None: # TODO: if probability?
            self.y_prob.append(probability)
        else:
            self.y_prob.append(prediction)

    def generate_report(self):
        """
        Calculates and prints standard industrial safety metrics.
        Returns: recall, accuracy, f1, auc
        """
        print("\n" + "="*40)
        print("ğŸ“Š MODEL PERFORMANCE REPORT (MVP)")
        print("="*40)

        # Calculate Core Metrics
        recall = recall_score(self.y_true, self.y_pred, zero_division=0)
        f1 = f1_score(self.y_true, self.y_pred, zero_division=0)
        acc = accuracy_score(self.y_true, self.y_pred)
        
        # Calculate ROC-AUC (Requires both classes to be present in data)
        try:
            auc = roc_auc_score(self.y_true, self.y_prob)
        except ValueError:
            auc = 0.0
            print("(!) Warning: Only one class present in data. AUC cannot be calculated.")

        # Console Output
        print(f"âœ… Recall (Safety Critical):      {recall*100:.2f}%")
        print(f"ğŸ¯ Accuracy (Baseline):           {acc*100:.2f}%") 
        print(f"âš–ï¸  F1 Score (Balance):           {f1*100:.2f}%")
        print(f"ğŸ“ˆ ROC-AUC Score:                 {auc:.4f}")
        
        print("\n" + "-"*40)
        print("ğŸ” Detailed Classification Report:")
        print(classification_report(self.y_true, self.y_pred, target_names=['Normal', 'Failure'], zero_division=0))
        
        return recall, acc, f1, auc

    def plot_confusion_matrix(self):
        """
        Visualizes the Confusion Matrix to highlight False Negatives.
        """
        cm = confusion_matrix(self.y_true, self.y_pred)
        
        plt.figure(figsize=(6, 5))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                    xticklabels=['Pred: Normal', 'Pred: Failure'],
                    yticklabels=['True: Normal', 'True: Failure'])
        
        plt.title('Confusion Matrix (Zero-Miss Target)')
        plt.ylabel('Ground Truth')
        plt.xlabel('Model Prediction')
        
        plt.tight_layout()
        plt.show()